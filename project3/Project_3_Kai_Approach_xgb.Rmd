---
title: "Project 3 - Kai Approach"
output: html_document
---

The vocabulary generation here follows code provided by Professor Liang on 
Piazza. Primary modification is that entire dataset contained in `alldata.tsv` 
is used to generate the vocabulary as opposed to individual folds.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
# Load packages
library(tidyverse)
library(ggplot2)
library(text2vec)
library(glmnet)
library(slam)
library(xgboost)
library(pROC)

set.seed(1129)
```


```{r}
#
# Vocabulary Generator
# Generate a vocabulary based on entire dataset
#
train = read.table("alldata.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
train$review = gsub('<.*?>', ' ', train$review)

stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "the", "us")
it_train = itoken(train$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)
tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))

it_train = itoken(train$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)
tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))
v.size = dim(dtm_train)[2]
ytrain = train$sentiment

summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)

n1 = sum(ytrain); 
n = length(ytrain)
n0 = n - n1

myp = (summ[,1] - summ[,3])/
  sqrt(summ[,2]/n1 + summ[,4]/n0)

words = colnames(dtm_train)
id = order(abs(myp), decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]
```

```{r}
#filtered
tmpfit = glmnet(x = dtm_train[, id], 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
myvocab1 = colnames(dtm_train[, id])[which(tmpfit$beta[, 41] != 0)]


# Unfiltered
tmpfit = glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
myvocab2 = colnames(dtm_train)[which(tmpfit$beta[, 42] != 0)]
```


```{r}
myvocab <- myvocab2
for (j in 1:5) {
    setwd(paste("split_", j, sep=""))
    train = read.table("train.tsv",
                       stringsAsFactors = FALSE,
                       header = TRUE)
    train$review <- gsub('<.*?>', ' ', train$review)
    it_train = itoken(train$review, 
                      preprocessor = tolower, 
                      tokenizer = word_tokenizer)
    vectorizer = vocab_vectorizer(create_vocabulary(myvocab, ngram = c(1L, 2L)))
    dtm_train = create_dtm(it_train, vectorizer)
    
    # mylogit.cv = cv.glmnet(x = dtm_train, 
    #                        y = train$sentiment, 
    #                        alpha = 0,
    #                        family='binomial', 
    #                        type.measure = "auc")
    # mylogit.fit = glmnet(x = dtm_train, 
    #                      y = train$sentiment, 
    #                      alpha = 0,
    #                      lambda = mylogit.cv$lambda.min, 
    #                      family='binomial')
    xgb.data <- xgb.DMatrix(data = as.matrix(dtm_train), label = train$sentiment)
    xgb.model <- xgboost(data = xgb.data,
                         #eta = 0.5, 
                         nrounds = 10, 
                         objective = "binary:logistic",
                         eval_metric = "auc",
                         booster = "gblinear",
                         alpha = 0.00001,
                         lambda = 0.00001,
                         verbose = 0) 
    test = read.table("test.tsv",
                      stringsAsFactors = FALSE,
                      header = TRUE)
    test$review <- gsub('<.*?>', ' ', test$review)
    it_test = itoken(test$review,
                     preprocessor = tolower, 
                     tokenizer = word_tokenizer)
    dtm_test = create_dtm(it_test, vectorizer)
    #mypred = predict(mylogit.fit, dtm_test, type = "response")
    mypred = predict(xgb.model, as.matrix(dtm_test), type = "response")
    output = data.frame(id = test$id, prob = as.vector(mypred))
    write.table(output, file = "mysubmission.txt", 
                row.names = FALSE, sep='\t')
    test.y = read.table("test_y.tsv", header = TRUE)
    pred = read.table("mysubmission.txt", header = TRUE)
    pred = merge(pred, test.y, by="id")
    roc_obj = roc(pred$sentiment, pred$prob)
    tmp = pROC::auc(roc_obj)
    print(tmp)
    setwd('..')
}
```

